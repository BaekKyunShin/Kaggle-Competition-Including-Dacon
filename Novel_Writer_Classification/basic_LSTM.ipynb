{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "8bs5PrgibQqv"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D, GlobalAveragePooling1D\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers import Dropout\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "import os\n",
    "import easydict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "LH8FDMMIRfan"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2.0.0\n"
     ]
    }
   ],
   "source": [
    "device = tf.test.gpu_device_name()\n",
    "print(device)\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "2E3P6KWQbV1g"
   },
   "outputs": [],
   "source": [
    "# 파일 불러오기\n",
    "train = pd.read_csv('open/train.csv', encoding='utf-8')\n",
    "test = pd.read_csv('open/test_x.csv', encoding='utf-8')\n",
    "sample_submission = pd.read_csv('open/sample_submission.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Z-Y3FBklbVvQ"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>He was almost choking. There was so much, so m...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>“Your sister asked for it, I suppose?”</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  author\n",
       "0  He was almost choking. There was so much, so m...       3\n",
       "1             “Your sister asked for it, I suppose?”       2"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.drop('index', axis=1, inplace=True)\n",
    "train.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "JrBuSlU6bVW-"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>“Not at all. I think she is one of the most ch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"No,\" replied he, with sudden consciousness, \"...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0  “Not at all. I think she is one of the most ch...\n",
       "1  \"No,\" replied he, with sudden consciousness, \"..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.drop('index', axis=1, inplace=True)\n",
    "test.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "PzmwLGS6bVUc"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index  0  1  2  3  4\n",
       "0      0  0  0  0  0  0\n",
       "1      1  0  0  0  0  0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_submission.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MLm37CheeO_e"
   },
   "source": [
    "# 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "WYbTUx7ldpIP"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\weroo/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# 불용어\n",
    "basic_stopwords = { \"a\", \"about\", \"above\", \"after\", \"again\", \"against\", \"all\", \"am\", \"an\", \"and\", \"any\", \"are\", \"as\", \n",
    "             \"at\", \"be\", \"because\", \"been\", \"before\", \"being\", \"below\", \"between\", \"both\", \"but\", \"by\", \"could\", \n",
    "             \"did\", \"do\", \"does\", \"doing\", \"down\", \"during\", \"each\", \"few\", \"for\", \"from\", \"further\", \"had\", \"has\", \n",
    "             \"have\", \"having\", \"he\", \"he'd\", \"he'll\", \"he's\", \"her\", \"here\", \"here's\", \"hers\", \"herself\", \"him\", \"himself\", \n",
    "             \"his\", \"how\", \"how's\", \"i\", \"i'd\", \"i'll\", \"i'm\", \"i've\", \"if\", \"in\", \"into\", \"is\", \"it\", \"it's\", \"its\", \"itself\", \n",
    "             \"let's\", \"me\", \"more\", \"most\", \"my\", \"myself\", \"nor\", \"of\", \"on\", \"once\", \"only\", \"or\", \"other\", \"ought\", \"our\", \"ours\", \n",
    "             \"ourselves\", \"out\", \"over\", \"own\", \"same\", \"she\", \"she'd\", \"she'll\", \"she's\", \"should\", \"so\", \"some\", \"such\", \"than\", \"that\", \n",
    "             \"that's\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\", \"there\", \"there's\", \"these\", \"they\", \"they'd\", \"they'll\", \n",
    "             \"they're\", \"they've\", \"this\", \"those\", \"through\", \"to\", \"too\", \"under\", \"until\", \"up\", \"very\", \"was\", \"we\", \"we'd\", \"we'll\", \n",
    "             \"we're\", \"we've\", \"were\", \"what\", \"what's\", \"when\", \"when's\", \"where\", \"where's\", \"which\", \"while\", \"who\", \"who's\", \"whom\", \n",
    "             \"why\", \"why's\", \"with\", \"would\", \"you\", \"you'd\", \"you'll\", \"you're\", \"you've\", \"your\", \"yours\", \"yourself\", \"yourselves\" }\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk_stopwords = set(stopwords.words('english'))\n",
    "\n",
    "final_stopwords = nltk_stopwords.union(basic_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "uR9Y1SX2bayy"
   },
   "outputs": [],
   "source": [
    "# 부호를 제거해주는 함수\n",
    "def alpha_num(text):\n",
    "    return re.sub(r'[^A-Za-z0-9 ]', '', text)\n",
    "\n",
    "# 불용어 제거해주는 함수\n",
    "def remove_stopwords(text: str):\n",
    "    final_text = []\n",
    "    for i in text.split():\n",
    "        if i.strip().lower() not in final_stopwords:\n",
    "            final_text.append(i.strip())\n",
    "    return \" \".join(final_text)\n",
    "    \n",
    "# 전처리 적용\n",
    "train['text']=train['text'].apply(alpha_num)\n",
    "test['text']=test['text'].apply(alpha_num)\n",
    "train['text'] = train['text'].str.lower()\n",
    "test['text'] = test['text'].str.lower()\n",
    "train['text'] = train['text'].apply(alpha_num).apply(remove_stopwords)\n",
    "test['text'] = test['text'].apply(alpha_num).apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "SkZqzH5Yff1Q"
   },
   "outputs": [],
   "source": [
    "# train test 분리\n",
    "# X_train = np.array([x for x in train['text']])\n",
    "# X_test = np.array([x for x in test['text']])\n",
    "# y_train = np.array([x for x in train['author']])\n",
    "\n",
    "X_train = train['text'].values\n",
    "X_test = test['text'].values\n",
    "y_train = train['author'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "fm8-p4Crf5Cm"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['almost choking much much wanted say strange exclamations came lips pole gazed fixedly bundle notes hand looked odin evident perplexity',\n",
       "       'sister asked suppose',\n",
       "       'engaged one day walked perusing janes last letter dwelling passages proved jane written spirits instead surprised mr odin saw looking odin meeting putting away letter immediately forcing smile said',\n",
       "       ..., 'sincere wellwisher friend sister lucy odin',\n",
       "       'wanted lend money', 'certainly occurred said yes like'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "3ZIjMq-Vhk77"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54879"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yZa_ZTOYeQec"
   },
   "source": [
    "# 토크나이징"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "KeZBCNHsba4-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 47118 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(num_words=30000, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~', lower=True, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "ipmUQeOpba19"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[135, 7244, 17, 17, 310, 18, 223, 3058, 32, 446, 2725, 1195, 5011, 1694, 1200, 41, 53, 2, 1207, 2818], [211, 54, 215], [675, 4, 55, 244, 13184, 4732, 34, 188, 4528, 4733, 1225, 479, 618, 493, 692, 495, 5, 2, 73, 101, 2, 664, 855, 46, 188, 418, 5310, 283, 3], [245, 5012, 867, 1230, 39, 8427, 823, 1497, 115, 248, 20, 1140, 610, 5311, 496, 2, 61, 1728, 129, 393, 2158, 1782, 2115, 1619, 610, 100, 3746, 6404, 1894, 131, 1783], [1374, 340, 2, 1548, 100, 27, 482, 1471, 968, 205, 2031, 98, 6651, 4000, 884, 16684, 6964, 9539, 77, 194], [9, 3410, 3, 3167, 1231, 20, 710], [655, 425, 1549, 388, 238, 258, 6405, 303, 70, 10], [1089, 595, 7, 25535, 3, 11, 937, 7, 6142, 68, 3344, 1761, 7, 12, 35, 651, 25536, 996, 696, 6, 1498, 297, 5149, 11983, 6965, 55, 737, 3002, 28, 80, 968, 16685, 31, 332, 25537, 5907, 1679], [34, 1968, 87, 3], [15, 170, 23, 849, 24, 1030, 5312, 268, 1916, 156, 79, 2013, 79, 2772, 79, 19831, 1333]]\n",
      "['almost choking much much wanted say strange exclamations came lips pole gazed fixedly bundle notes hand looked odin evident perplexity'\n",
      " 'sister asked suppose'\n",
      " 'engaged one day walked perusing janes last letter dwelling passages proved jane written spirits instead surprised mr odin saw looking odin meeting putting away letter immediately forcing smile said'\n",
      " 'captain porch keeping carefully way treacherous shot intended turned spoke us doctors watch lookout dr odin take north side please jim east gray west watch hands load muskets lively men careful'\n",
      " 'mercy gentlemen odin flung hands dont write anyway shame ive torn heart asunder seize opportunity fingering wounds halves oh god'\n",
      " 'well fought said sooth charge us twice'\n",
      " 'pay impossible considering character talk fellow plague mine another time'\n",
      " 'proper figure man atarms said little knight man chicken yet warrant stronger man see great stone coping hath fodin upon bridge four lazy varlets strove day carry hence two put shame budging though fear overtask grievous weight'\n",
      " 'last sunday night said'\n",
      " 'must ask cried hell may noble flames known score years always hated always admired always slavishly feared']\n"
     ]
    }
   ],
   "source": [
    "X_token = tokenizer.texts_to_sequences(X_train)\n",
    "print(X_token[:10])\n",
    "print(X_train[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "Pr2xHcecbavk"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(54879, 500)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_token = pad_sequences(X_token, maxlen=500, padding='post', truncating='post')\n",
    "X_token.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "9bZ66A1sbVO5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19617, 500)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_token_test = tokenizer.texts_to_sequences(X_test)\n",
    "X_token_test = pad_sequences(X_token_test, maxlen=500, padding='post', truncating='post')\n",
    "X_token_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "kBDctcWybQiA"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 1, 0],\n",
       "       [0, 0, 1, 0, 0],\n",
       "       [0, 1, 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 1, 0, 0, 0],\n",
       "       [0, 0, 0, 1, 0],\n",
       "       [1, 0, 0, 0, 0]], dtype=uint8)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train = pd.get_dummies(y_train).values\n",
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "fAb0iBUmmp4F",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(49391, 500) (49391, 5)\n",
      "(5488, 500) (5488, 5)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X_token, y_train, test_size=0.1, random_state=42)\n",
    "print(X_train.shape, Y_train.shape)\n",
    "print(X_test.shape, Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "5EK3b0f8nTWs"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 500, 20)           600000    \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_1 (Spatial (None, 500, 20)           0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 20)                3280      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 16)                336       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 5)                 85        \n",
      "=================================================================\n",
      "Total params: 603,701\n",
      "Trainable params: 603,701\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model=Sequential()\n",
    "model.add(Embedding(30000, 20, input_length=500))\n",
    "# model.add(GlobalAveragePooling1D())\n",
    "model.add(SpatialDropout1D(0.1))\n",
    "model.add(LSTM(20, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dense(5, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nsakBgFLnTd-"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\weroo\\Anaconda\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 44451 samples, validate on 4940 samples\n",
      "Epoch 1/20\n",
      "44451/44451 [==============================] - 705s 16ms/step - loss: 1.5725 - accuracy: 0.2729 - val_loss: 1.5767 - val_accuracy: 0.2719\n",
      "Epoch 2/20\n",
      "44128/44451 [============================>.] - ETA: 5s - loss: 1.5704 - accuracy: 0.2744"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, Y_train, \n",
    "                    epochs=20,\n",
    "                    validation_split=0.1, \n",
    "                    )\n",
    "# callbacks=[EarlyStopping(monitor='val_loss', patience=2, min_delta=0.0001)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zkx-I2SjnTPZ"
   },
   "outputs": [],
   "source": [
    "accr = model.evaluate(X_test,Y_test)\n",
    "print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nfFrUBmuo64f"
   },
   "outputs": [],
   "source": [
    "plt.title('Loss')\n",
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='test')\n",
    "plt.legend()\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Nu-Z7_nio7CS"
   },
   "outputs": [],
   "source": [
    "plt.title('Accuracy')\n",
    "plt.plot(history.history['accuracy'], label='train')\n",
    "plt.plot(history.history['val_accuracy'], label='test')\n",
    "plt.legend()\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FQN-3LcQPlMl"
   },
   "outputs": [],
   "source": [
    "pred = model.predict_proba(X_token_test)\n",
    "\n",
    "# submission\n",
    "sample_submission[['0','1','2','3','4']] = pred\n",
    "sample_submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8CDw2Yx3o6-B"
   },
   "outputs": [],
   "source": [
    "sample_submission.to_csv('submission_temp.csv', index = False, encoding = 'utf-8')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "basic_LSTM.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
